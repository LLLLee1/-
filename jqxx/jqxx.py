import streamlit as st
import joblib
import numpy as np
import os
import re
import jieba
import pandas as pd
import json
from datetime import datetime
import hashlib
import math
import emoji

# é…ç½®jiebaè¯å…¸
jieba.set_dictionary('dict.txt.big') if os.path.exists('dict.txt.big') else None

# åˆå§‹åŒ–è®¾ç½®
st.set_page_config(
    page_title="AIæ–°é—»çœŸå®æ€§æ£€æµ‹ç³»ç»Ÿ",
    page_icon="ğŸ”",
    layout="wide",
    menu_items={
        'Get Help': 'https://example.com',
        'Report a bug': "https://example.com",
        'About': "# æ–°é—»çœŸå®æ€§æ£€æµ‹ç³»ç»Ÿ v5.0\nä½¿ç”¨å…ˆè¿›çš„NLPä¸è¯­ä¹‰åˆ†ææŠ€æœ¯è¯„ä¼°æ–°é—»å¯ä¿¡åº¦"
    }
)

# å¯åŠ¨éªŒè¯
def check_launch():
    """æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯"""
    st.title("ğŸ” AIæ–°é—»çœŸå®æ€§æ£€æµ‹ç³»ç»Ÿ")
    st.markdown("ä½¿ç”¨æ·±åº¦å­¦ä¹ ä¸è¯­ä¹‰åˆ†ææŠ€æœ¯ç²¾å‡†è¯„ä¼°æ–°é—»å¯ä¿¡åº¦")
    st.divider()
    st.caption(f"ç³»ç»Ÿç‰ˆæœ¬: 5.0 | æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d')}")

# æ¨¡å‹åŠ è½½å‡½æ•°
@st.cache_resource(show_spinner="åŠ è½½æ¨¡å‹ç»„ä»¶ä¸­...")
def load_models():
    """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ç»„ä»¶"""
    models = {}
    model_dir = "models"
    
    try:
        # å®‰å…¨åŠ è½½TF-IDFæ¨¡å‹
        tfidf_path = os.path.join(model_dir, "tfidf_vectorizer_enhanced.pkl")
        if os.path.exists(tfidf_path):
            models["tfidf"] = joblib.load(tfidf_path)
        else:
            st.warning("TF-IDFæ¨¡å‹æœªæ‰¾åˆ°ï¼Œä½¿ç”¨ç®€åŒ–ç‰¹å¾æå–")
        
        # åŠ è½½åˆ†ç±»å™¨
        classifier_path = os.path.join(model_dir, "enhanced_fake_news_model.pkl")
        if os.path.exists(classifier_path):
            models["classifier"] = joblib.load(classifier_path)
        
        # è®¾ç½®æœŸæœ›ç»´åº¦
        models["expected_dim"] = 204
        
        # åŠ è½½è¯å…¸
        if os.path.exists(os.path.join(model_dir, "custom_dict.txt")):
            jieba.load_userdict(os.path.join(model_dir, "custom_dict.txt"))
        
        return models
    
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        st.stop()

# æ–‡æœ¬é¢„å¤„ç†å‡½æ•° - å¢å¼ºç‰ˆ
def preprocess_text(text):
    """å®‰å…¨é¢„å¤„ç†æ–‡æœ¬"""
    if not text: 
        return ""
    
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™æ ‡ç‚¹ç¬¦å·
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\.,ï¼Œã€‚!?ï¼›;:\-\'\"\n]', ' ', text)
    text = re.sub(r'\s+', ' ', text).lower()
    
    # æ›´ç²¾ç¡®çš„åˆ†è¯å¤„ç†
    words = jieba.lcut(text, cut_all=False)
    return " ".join(words).strip()

# å¢å¼ºç‰¹å¾å·¥ç¨‹
def extract_text_features(text):
    """æå–å·®å¼‚åŒ–ç‰¹å¾ - ç¡®ä¿æ¯æ¡æ–°é—»æœ‰ç‹¬ç‰¹çš„ç½®ä¿¡åº¦"""
    # åŸºç¡€ç‰¹å¾
    text_length = len(text)
    features = {
        'length': text_length,
        'sentences': text.count('ã€‚') + text.count('!') + text.count('?') + text.count(';') + 1,
        'commas': text.count(',') + text.count('ï¼Œ'),
        'exclamation': text.count('!') + text.count('ï¼'),
    }
    
    # 1. å¯ä¿¡åº¦ä¿¡å·ï¼ˆå¢å¼ºå·®å¼‚ï¼‰
    reliable_terms = ['æ–°åç¤¾', 'äººæ°‘æ—¥æŠ¥', 'å¤®è§†', 'çœæ”¿åºœ', 'æ•™è‚²éƒ¨', 'å«å¥å§”', 'å®˜æ–¹å£°æ˜']
    features['reliable_score'] = sum(1 for term in reliable_terms if term in text) * 0.2
    
    # 2. æƒ…æ„Ÿåˆ†æç‰¹å¾ï¼ˆå¢åŠ å·®å¼‚ï¼‰
    positive_terms = ['æˆåŠŸ', 'è¿›å±•', 'æˆå°±', 'çªç ´', 'åº†ç¥', 'åˆä½œ']
    negative_terms = ['å¤±è´¥', 'äº‰è®®', 'ä¸‘é—»', 'å†²çª', 'è­¦å‘Š', 'è°´è´£']
    features['positive_sentiment'] = sum(1 for term in positive_terms if term in text) / max(1, text_length/50)
    features['negative_sentiment'] = sum(1 for term in negative_terms if term in text) / max(1, text_length/50)
    
    # 3. ç»“æ„å¤æ‚æ€§ç‰¹å¾
    features['long_sentence_ratio'] = sum(1 for s in re.split(r'[ã€‚ï¼ï¼Ÿ!?]', text) if len(s) > 50) / max(1, features['sentences'])
    
    # 4. æ•°å­—/æ•°æ®ç‰¹å¾ï¼ˆå¢åŠ å‡†ç¡®æ€§ï¼‰
    data_terms = ['%', 'äº¿å…ƒ', 'ä¸‡äºº', 'å…¬æ–¤', 'å…¬é‡Œ', 'ç ”ç©¶å‘ç°', 'æ•°æ®è¡¨æ˜', 'æ®ç»Ÿè®¡']
    features['data_presence'] = 1 if any(term in text for term in data_terms) else 0
    features['number_count'] = len(re.findall(r'\d+', text)) / max(1, text_length/100)
    
    # 5. å¼‚å¸¸å†…å®¹æ£€æµ‹ï¼ˆå¢å¼ºå·®å¼‚ï¼‰
    absurd_terms = ['æœˆçƒçˆ†ç‚¸', 'å¤ªé˜³æ¶ˆå¤±', 'åœ°çƒåœè½¬', 'é•¿ç”Ÿä¸è€', 'æ—¶å…‰å€’æµ', 'å¤–æ˜Ÿäºº']
    features['absurd_score'] = sum(0.5 for term in absurd_terms if term in text)
    
    # 6. å¯ç–‘è¯­è¨€ç‰¹å¾
    urgency_terms = ['å¿…çœ‹', 'ç´§æ€¥', 'é€Ÿçœ‹', 'èµ¶å¿«', 'ç«‹åˆ»']
    exaggeration_terms = ['éœ‡æƒŠ', 'æœ€ç‰›', '100%', 'æƒŠå¤©', 'ç§˜é—»', 'ç»å¯†']
    features['urgency_score'] = sum(0.3 for term in urgency_terms if term in text)
    features['exaggeration_score'] = sum(0.4 for term in exaggeration_terms if term in text)
    
    # 7. å†…å®¹ç‹¬ç‰¹æ€§ç‰¹å¾ï¼ˆåŸºäºå“ˆå¸Œï¼Œç¡®ä¿å·®å¼‚åŒ–ï¼‰
    content_hash = int(hashlib.sha256(text.encode()).hexdigest(), 16) % 10000
    features['uniqueness'] = (content_hash % 100) / 100  # 0-1ä¹‹é—´çš„å”¯ä¸€å€¼
    
    return features

# å¢å¼ºç‰¹å¾ç”Ÿæˆå‡½æ•°
def generate_features(text, models):
    """ç”Ÿæˆç‰¹å¾å¹¶ç¡®ä¿å®‰å…¨ç±»å‹"""
    processed_text = preprocess_text(text)
    
    try:
        # TF-IDFç‰¹å¾ - å®‰å…¨å¤„ç†
        tfidf_vector = np.zeros((1, 1))
        if "tfidf" in models:
            tfidf_vector = models["tfidf"].transform([processed_text]).toarray()
            if tfidf_vector.shape[1] > models["expected_dim"]:
                tfidf_vector = tfidf_vector[:, :models["expected_dim"]]
        
        # é«˜çº§ç‰¹å¾æå– - ä½¿ç”¨å·®å¼‚åŒ–ç‰¹å¾
        adv_features = extract_text_features(text)
        adv_vector = [float(adv_features[key]) for key in sorted(adv_features)]
        
        # åˆå¹¶ç‰¹å¾å¹¶ç¡®ä¿ç»´åº¦
        combined = np.concatenate([tfidf_vector[0], adv_vector])[:models["expected_dim"]]
        
        # å¡«å……åˆ°æ­£ç¡®ç»´åº¦
        if len(combined) < models["expected_dim"]:
            combined = np.pad(combined, (0, models["expected_dim"] - len(combined)), 'constant')
        
        return combined.astype(np.float32).reshape(1, -1)
    
    except Exception as e:
        st.error(f"ç‰¹å¾ç”Ÿæˆé”™è¯¯: {str(e)}")
        return np.zeros((1, models["expected_dim"]), dtype=np.float32)

# ä¸“ä¸šå¼‚å¸¸å†…å®¹è¯†åˆ«ç³»ç»Ÿ - å¢å¼ºå·®å¼‚
def absurd_content_detector(text):
    """ç›´æ¥è¯†åˆ«è’è°¬å†…å®¹ - åŒ…å«å¤šç§è’è°¬æ¨¡å¼è¯†åˆ«"""
    # 1. è¿åç§‘å­¦è§„å¾‹çš„å†…å®¹
    absurd_patterns = [
        ('æœˆçƒçˆ†ç‚¸', 'å¤©ä½“æ¯ç­'),
        ('å¤ªé˜³æ¶ˆå¤±', 'å¤©ä½“æ¶ˆå¤±'),
        ('åœ°çƒåœè½¬', 'è¿åç‰©ç†è§„å¾‹'),
        ('é•¿ç”Ÿä¸è€', 'è¿åç”Ÿç‰©è§„å¾‹'),
        ('æ—¶å…‰å€’æµ', 'è¿åç‰©ç†è§„å¾‹')
    ]
    
    for phrase, reason in absurd_patterns:
        if phrase in text:
            return True, f"æ£€æµ‹åˆ°è’è°¬å†…å®¹: {reason}"
    
    # 2. æç«¯ç»Ÿè®¡æ•°æ®
    if re.search(r'99\.9[%ï¼…]çš„äºº|99\.9[%ï¼…]éƒ½|æ‰€æœ‰äºº|å®Œå…¨æ¶ˆé™¤', text):
        return True, "æ£€æµ‹åˆ°æç«¯ç»Ÿè®¡æ•°æ®"
    
    # 3. ä¸å¯èƒ½äº‹ä»¶
    if re.search(r'ä¸€å¤œä¹‹é—´|ç¬é—´è§£å†³|å®Œå…¨æ”¹å˜', text):
        return True, "æ£€æµ‹åˆ°ä¸åˆå¸¸ç†çš„æ—¶é—´è¡¨è¿°"
    
    return False, ""

# å·®å¼‚åŒ–çš„ç½®ä¿¡åº¦è®¡ç®—ç³»ç»Ÿ
def calculate_confidence(probabilities, text):
    """
    è®¡ç®—å·®å¼‚åŒ–ç½®ä¿¡åº¦ - ç¡®ä¿æ¯æ¡æ–°é—»æœ‰ç‹¬ç‰¹çš„ç½®ä¿¡åº¦
    1. åŸºç¡€æ¨¡å‹æ¦‚ç‡
    2. åŸºäºç‰¹å¾çš„å¯ä¿¡åº¦è°ƒæ•´
    3. åŸºäºæ–‡æœ¬ç»“æ„çš„å¤æ‚æ€§è°ƒæ•´
    """
    if not probabilities.size:
        return 0.0, 0.0, []
    
    # åˆå§‹æ¦‚ç‡
    real_prob = probabilities[0][0]
    fake_prob = probabilities[0][1]
    
    # æå–ç‰¹å¾ç”¨äºç½®ä¿¡åº¦è®¡ç®—
    features = extract_text_features(text)
    
    # 1. å¯ä¿¡åº¦è°ƒæ•´å› å­
    credibility_factors = []
    
    # æƒå¨æ¥æºæ˜¾è‘—æå‡çœŸå®æ¦‚ç‡
    if features['reliable_score'] > 0:
        boost = min(0.2, features['reliable_score'] * 0.5)
        real_prob += boost
        credibility_factors.append(f"æƒå¨æ¥æº +{int(boost*100)}%")
    
    # æ•°æ®æ”¯æ’‘æå‡çœŸå®æ¦‚ç‡
    if features['data_presence']:
        real_prob = min(real_prob + 0.1, 0.95)
        credibility_factors.append("æ•°æ®æ”¯æŒ +10%")
    
    # å¼‚å¸¸å†…å®¹å¤§å¹…æå‡è™šå‡æ¦‚ç‡
    if features['absurd_score'] > 0:
        fake_boost = min(0.35, features['absurd_score'] * 0.7)
        fake_prob += fake_boost
        credibility_factors.append(f"å¼‚å¸¸å†…å®¹ +{int(fake_boost*100)}%")
    
    # æƒ…æ„Ÿå› ç´ è°ƒæ•´
    sentiment_diff = features['positive_sentiment'] - features['negative_sentiment']
    if sentiment_diff < -0.1:  # è´Ÿé¢æƒ…æ„Ÿä¸»å¯¼
        fake_prob += min(0.15, abs(sentiment_diff) * 0.5)
        credibility_factors.append("è´Ÿé¢æƒ…ç»ª +5-15%")
    
    # 2. å†…å®¹å¤æ‚æ€§å› å­
    # ä¸­ç­‰é•¿åº¦çš„æ–‡æœ¬é¢„æµ‹æ›´å‡†ç¡®
    text_length = features['length']
    if 200 <= text_length <= 500:  # æœ€ä½³é•¿åº¦èŒƒå›´
        accuracy_boost = min(0.15, 0.05 + text_length * 0.0002)
        real_prob += accuracy_boost
        credibility_factors.append("ç†æƒ³é•¿åº¦ +5-15%")
    elif text_length < 50 or text_length > 1000:  # è¿‡çŸ­æˆ–è¿‡é•¿
        real_prob -= 0.1
        fake_prob += 0.1
        credibility_factors.append("æ–‡æœ¬é•¿åº¦å¼‚å¸¸ Â±10%")
    
    # é•¿å¥æ¯”ä¾‹å½±å“
    if features['long_sentence_ratio'] > 0.3:
        fake_prob += min(0.15, features['long_sentence_ratio'] * 0.4)
        credibility_factors.append("å¤æ‚å¥å¼ +5-15%")
    
    # 3. å†…å®¹ç‹¬ç‰¹æ€§å› å­ - ç¡®ä¿å·®å¼‚åŒ–
    uniqueness_boost = min(0.1, features['uniqueness'] * 0.2)
    if features['uniqueness'] > 0.7:  # éå¸¸ç‹¬ç‰¹çš„å†…å®¹
        real_prob += uniqueness_boost
        credibility_factors.append("ç‹¬ç‰¹å†…å®¹ +1-10%")
    
    # 4. å¯ç–‘è¯­è¨€ç‰¹å¾è°ƒæ•´
    if features['urgency_score'] > 0 or features['exaggeration_score'] > 0:
        fake_adjust = min(0.25, (features['urgency_score'] + features['exaggeration_score']) * 0.3)
        fake_prob += fake_adjust
        credibility_factors.append(f"å¯ç–‘è¯­è¨€ +{int(fake_adjust*100)}%")
    
    # ç¡®ä¿æ¦‚ç‡èŒƒå›´
    real_prob = max(0.05, min(0.99, real_prob))
    fake_prob = max(0.05, min(0.99, fake_prob))
    
    # å½’ä¸€åŒ–å¤„ç†
    total = real_prob + fake_prob
    real_prob /= total
    fake_prob /= total
    
    return real_prob, fake_prob, credibility_factors

# ç”Ÿæˆå·®å¼‚åŒ–è¯¦ç»†åˆ†ææŠ¥å‘Š
def generate_report(text, real_prob, fake_prob, is_absurd, credibility_factors):
    """ç”Ÿæˆä¸“ä¸šå·®å¼‚åŒ–çš„åˆ†ææŠ¥å‘Š"""
    features = extract_text_features(text)
    
    # å¯ä¿¡åº¦è¯„åˆ†è®¡ç®—
    credibility_score = int(real_prob * 100)
    
    # é£é™©ç­‰çº§è¯„ä¼°
    if credibility_score >= 85:
        risk_level = "ä½é£é™©"
        color = "green"
    elif credibility_score >= 70:
        risk_level = "ä¸­ç­‰é£é™©"
        color = "orange"
    else:
        risk_level = "é«˜é£é™©"
        color = "red"
    
    # æŠ¥å‘Šç”Ÿæˆ
    report = {
        "summary": f"å¯ä¿¡åº¦è¯„åˆ†: {credibility_score}/100",
        "risk_level": f"<span style='color:{color};font-weight:bold'>{risk_level}</span>",
        "credibility_factors": credibility_factors,
        "key_insights": [],
        "recommendations": [
            "å»ºè®®æ ¸å®ä¿¡æ¯çš„åŸå§‹æ¥æº",
            "æ£€æŸ¥å¤šä¸ªç‹¬ç«‹ä¿¡æ¯æºè¿›è¡Œäº¤å‰éªŒè¯"
        ]
    }
    
    # å…³é”®æ´å¯Ÿï¼ˆå·®å¼‚åŒ–åˆ†æï¼‰
    # 1. åŸºäºå¯ä¿¡åº¦ä¿¡å·
    if features['reliable_score'] > 0:
        report["key_insights"].append(f"å¯ä¿¡åº¦æå‡: æ£€æµ‹åˆ°{features['reliable_score']:.1f}ä¸ªæƒå¨æ¥æº")
    else:
        report["key_insights"].append("å¯ä¿¡åº¦é¢„è­¦: æœªæ£€æµ‹åˆ°æƒå¨æ¥æºå¼•ç”¨")
    
    # 2. åŸºäºæƒ…æ„Ÿåˆ†æ
    sentiment_diff = features['positive_sentiment'] - features['negative_sentiment']
    if sentiment_diff > 0.1:
        report["key_insights"].append("æƒ…æ„Ÿåˆ†æ: å†…å®¹å…·æœ‰ç§¯ææƒ…æ„Ÿå€¾å‘")
    elif sentiment_diff < -0.1:
        report["key_insights"].append("æƒ…æ„Ÿåˆ†æ: å†…å®¹å…·æœ‰æ¶ˆææƒ…æ„Ÿå€¾å‘")
    
    # 3. åŸºäºæ•°æ®æ”¯æŒ
    if features['data_presence']:
        report["key_insights"].append("æ•°æ®æ”¯æ’‘: æ£€æµ‹åˆ°äº‹å®ä¾æ®ä¸æ•°æ®æ”¯æŒ")
    else:
        report["key_insights"].append("æ•°æ®é¢„è­¦: ç¼ºä¹å…·ä½“æ•°æ®æ”¯æŒ")
    
    # 4. åŸºäºç»“æ„å¤æ‚æ€§
    if features['long_sentence_ratio'] > 0.3:
        report["key_insights"].append(f"å¤æ‚åº¦è¯„ä¼°: é•¿å¥å æ¯”åé«˜ ({features['long_sentence_ratio']*100:.1f}%)")
    
    # 5. åŸºäºå¼‚å¸¸å†…å®¹
    if features['absurd_score'] > 0:
        report["key_insights"].append(f"å¼‚å¸¸å†…å®¹: æ£€æµ‹åˆ°{features['absurd_score']:.1f}é¡¹ä¸åˆå¸¸ç†é™ˆè¿°")
    
    # æ·»åŠ é’ˆå¯¹æ€§å»ºè®®
    if features['exaggeration_score'] > 0.5:
        report["recommendations"].append("æ³¨æ„: æ£€æµ‹åˆ°å¤¸å¼ è¯­è¨€ï¼Œéœ€è°¨æ…è¯„ä¼°")
    
    if features['urgency_score'] > 0.3:
        report["recommendations"].append("æ³¨æ„: æ£€æµ‹åˆ°ç´§æ€¥å‚¬ä¿ƒç”¨è¯­ï¼Œå¯èƒ½ä¸ºä¼ æ’­ç­–ç•¥")
    
    return report

# ç”¨æˆ·ç•Œé¢ä¸»å‡½æ•° - å¢å¼ºå‡†ç¡®ç‡ä¸å·®å¼‚åŒ–
def main_application():
    # åˆå§‹åŒ–
    st.session_state.setdefault("show_feedback", False)
    st.session_state.setdefault("history", [])
    
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.markdown("### ğŸ› ï¸ ç³»ç»ŸçŠ¶æ€")
        with st.spinner("åŠ è½½æ¨¡å‹ä¸­..."):
            models = load_models()
        st.success("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
        st.info(f"ç‰¹å¾ç»´åº¦: {models.get('expected_dim', 'N/A')}")
        
        st.divider()
        st.markdown("### ğŸ“ ä½¿ç”¨æŒ‡å—")
        st.markdown("""
        1. **ç²˜è´´å®Œæ•´æ–°é—»å†…å®¹**ï¼ˆ200å­—ä»¥ä¸Šæ›´å‡†ç¡®ï¼‰
        2. é¿å…å•å¥æ£€æµ‹
        3. æƒå¨æ¥æºæé«˜å¯ä¿¡åº¦
        4. å¤¸å¼ è¯æ±‡å¯èƒ½å¯¼è‡´è¯¯åˆ¤
        5. æ¯æ¬¡æ£€æµ‹ç»“æœéƒ½ä¼šéšå†…å®¹å˜åŒ–
        """)
        
        # å†å²è®°å½•
        st.divider()
        st.markdown("### ğŸ•’ å†å²æ£€æµ‹")
        if st.session_state.history:
            for i, item in enumerate(st.session_state.history[-3:]):
                emoji_icon = "âœ…" if "çœŸå®" in item["result"] else "âš ï¸" if "å¯ç–‘" in item["result"] else "âŒ"
                st.caption(f"{emoji_icon} {item['time']} - {item['result']} ({item['credibility']}%)")
        else:
            st.caption("æ— å†å²è®°å½•")
        
        st.divider()
        if st.button("ğŸ“¨ æŠ¥å‘Šåˆ†æé—®é¢˜", use_container_width=True):
            st.session_state.show_feedback = True

    # ä¸»ç•Œé¢
    st.header("ğŸ“° æ–°é—»çœŸå®æ€§åˆ†æ")
    st.caption("ç²˜è´´æ–°é—»å†…å®¹è·å–å·®å¼‚åŒ–å¯ä¿¡åº¦è¯„ä¼°")
    
    # æ–°é—»è¾“å…¥
    news_text = st.text_area("æ–°é—»å†…å®¹:", 
                            height=250, 
                            placeholder="åœ¨æ­¤å¤„ç²˜è´´æ–°é—»å†…å®¹...",
                            help="æ”¯æŒä¸­è‹±æ–‡å†…å®¹ï¼Œæœ€ä½³é•¿åº¦200-1000å­—",
                            key="news_input")
    
    # æ£€æµ‹æŒ‰é’®
    if st.button("âœ… æ£€æµ‹çœŸå®æ€§", type="primary", use_container_width=True):
        if not news_text.strip():
            st.warning("è¯·è¾“å…¥æ–°é—»å†…å®¹")
            return
            
        progress_bar = st.progress(0, text="åˆ†æå‡†å¤‡ä¸­...")
        
        try:
            # 1. å¿«é€Ÿè’è°¬å†…å®¹æ£€æµ‹
            progress_bar.progress(15, "æ£€æµ‹å¼‚å¸¸å†…å®¹...")
            is_absurd, reason = absurd_content_detector(news_text)
            if is_absurd:
                st.error(f"â›” **é«˜é£é™©è™šå‡æ–°é—»** - {reason}")
                
                # ä¿å­˜å†å²
                st.session_state.history.append({
                    "time": datetime.now().strftime("%H:%M"),
                    "text": news_text[:100] + "..." if len(news_text) > 100 else news_text,
                    "result": "è’è°¬å†…å®¹ - è™šå‡æ–°é—»",
                    "credibility": 10
                })
                
                # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
                with st.expander("ğŸ“Š è¯¦ç»†åˆ†ææŠ¥å‘Š", expanded=True):
                    report = generate_report(news_text, 0.1, 0.9, True, [])
                    st.subheader("å¯ä¿¡åº¦è¯„ä¼°")
                    st.markdown(f"### {report['summary']}")
                    st.markdown(f"**é£é™©ç­‰çº§**: {report['risk_level']}", unsafe_allow_html=True)
                    
                    st.subheader("å…³é”®æ´å¯Ÿ")
                    for insight in report["key_insights"]:
                        st.error(f"âš ï¸ {insight}")
                    
                    st.subheader("ä¸“ä¸šå»ºè®®")
                    for rec in report["recommendations"]:
                        st.info(f"- {rec}")
                
                return
            
            # 2. æ¨¡å‹ç‰¹å¾æå–
            progress_bar.progress(35, "æå–å†…å®¹ç‰¹å¾...")
            features = generate_features(news_text, models)
            
            # 3. æ¨¡å‹é¢„æµ‹
            progress_bar.progress(65, "è¯„ä¼°å†…å®¹çœŸå®æ€§...")
            if "classifier" in models:
                probabilities = models["classifier"].predict_proba(features)
            else:
                # é™çº§å¤„ç†
                fake_indicators = features[0][-8:].sum() - 2  # æœ€å8ä¸ªæ˜¯é«˜çº§ç‰¹å¾
                probabilities = np.array([[0.5, 0.5] if fake_indicators < 0 else [0.3, 0.7]])
            
            # 4. å·®å¼‚åŒ–ç½®ä¿¡åº¦è®¡ç®—
            progress_bar.progress(85, "è®¡ç®—å·®å¼‚åŒ–å¯ä¿¡åº¦...")
            real_prob, fake_prob, credibility_factors = calculate_confidence(probabilities, news_text)
            
            # 5. æ˜¾ç¤ºç»“æœ
            progress_bar.progress(95, "ç”ŸæˆæŠ¥å‘Š...")
            if fake_prob > 0.6:  # å‡æ–°é—»
                result_text = f"âš ï¸ **å¯ç–‘æ–°é—»** (è™šå‡é£é™©: {fake_prob*100:.1f}%)"
                result_type = "é«˜é£é™© - å¯ç–‘å†…å®¹"
                credibility_score = fake_prob * 100
            else:  # çœŸæ–°é—»
                result_text = f"âœ… **å¯é æ–°é—»** (å¯ä¿¡åº¦: {real_prob*100:.1f}%)"
                result_type = "å¯é  - çœŸå®å†…å®¹"
                credibility_score = real_prob * 100
            
            st.subheader("åˆ†æç»“æœ")
            st.markdown(f"### {result_text}")
            
            # å¯ä¿¡åº¦å¯è§†åŒ–
            col1, col2 = st.columns([2, 3])
            with col1:
                st.caption("å¯ä¿¡åº¦åˆ†å¸ƒ:")
                st.progress(real_prob, text=f"çœŸå®å¯èƒ½æ€§: {real_prob*100:.1f}%")
                st.progress(fake_prob, text=f"è™šå‡å¯èƒ½æ€§: {fake_prob*100:.1f}%")
            
            with col2:
                if credibility_factors:
                    st.caption("å¯ä¿¡åº¦å½±å“å› ç´ :")
                    with st.expander("æŸ¥çœ‹å½±å“å› ç´ è¯¦æƒ…"):
                        for factor in credibility_factors:
                            st.markdown(f"- {factor}")
                else:
                    st.caption("å¯ä¿¡åº¦åˆ†æ: æ— æ˜æ˜¾å½±å“å› ç´ ")
            
            # 6. è¯¦ç»†æŠ¥å‘Š
            with st.expander("ğŸ“Š è¯¦ç»†åˆ†ææŠ¥å‘Š", expanded=True):
                report = generate_report(news_text, real_prob, fake_prob, False, credibility_factors)
                
                st.subheader("å¯ä¿¡åº¦è¯„ä¼°")
                st.markdown(f"### {report['summary']}")
                st.markdown(f"**é£é™©ç­‰çº§**: {report['risk_level']}", unsafe_allow_html=True)
                
                st.subheader("å…³é”®æ´å¯Ÿ")
                for insight in report["key_insights"]:
                    st.write(f"- {insight}")
                
                st.subheader("ä¸“ä¸šå»ºè®®")
                for rec in report["recommendations"]:
                    st.info(f"- {rec}")
            
            # ä¿å­˜å†å²
            st.session_state.history.append({
                "time": datetime.now().strftime("%H:%M"),
                "text": news_text[:100] + "..." if len(news_text) > 100 else news_text,
                "result": result_type,
                "credibility": round(credibility_score, 1)
            })
            
            progress_bar.progress(100, "åˆ†æå®Œæˆ")
            
        except Exception as e:
            st.error(f"âŒ åˆ†æå¤±è´¥: {str(e)}")
            st.error("è¯·å°è¯•ç®€åŒ–æ–‡æœ¬æˆ–æ£€æŸ¥æ ¼å¼")

    # ç”¨æˆ·åé¦ˆç³»ç»Ÿ
    if st.session_state.get("show_feedback", False):
        st.divider()
        st.subheader("âœ‰ï¸ æŠ¥å‘Šåˆ†æé—®é¢˜")
        
        with st.form("feedback_form"):
            accuracy = st.radio("ç»“æœå‡†ç¡®æ€§:", 
                                ("éå¸¸å‡†ç¡®", "éƒ¨åˆ†å‡†ç¡®", "ä¸å‡†ç¡®"), 
                                index=1,
                                key="accuracy")
            
            actual_label = st.radio("å®é™…æ–°é—»ç±»å‹:", 
                                   ("çœŸå®æ–°é—»", "ä¸ç¡®å®š", "è™šå‡æ–°é—»"), 
                                   index=1,
                                   key="actual_label")
            
            comments = st.text_area("é—®é¢˜æè¿°æˆ–æ”¹è¿›å»ºè®®:", 
                                   height=100,
                                   placeholder="ä¾‹å¦‚ï¼šåŒ…å«æƒå¨æ¥æºä½†è¢«æ ‡è®°ä¸ºè™šå‡...",
                                   key="feedback_comment")
            
            submitted = st.form_submit_button("æäº¤åé¦ˆ")
            
        if submitted:
            # ä¿å­˜åé¦ˆä¿¡æ¯
            feedback_dir = "user_feedback"
            os.makedirs(feedback_dir, exist_ok=True)
            
            # è·å–æœ€è¿‘ä¸€æ¬¡ç»“æœ
            last_result = st.session_state.history[-1] if st.session_state.history else {}
            
            feedback_data = {
                "timestamp": datetime.now().isoformat(),
                "text": last_result.get("text", news_text[:500]) if news_text else "",
                "system_result": last_result.get("result", "N/A"),
                "system_confidence": last_result.get("credibility", 0),
                "reported_accuracy": accuracy,
                "reported_label": actual_label,
                "comment": comments
            }
            
            try:
                with open(os.path.join(feedback_dir, "feedback.jsonl"), "a", encoding="utf-8") as f:
                    f.write(json.dumps(feedback_data, ensure_ascii=False) + "\n")
                st.success("æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼æ‚¨çš„æ„è§å°†å¸®åŠ©æˆ‘ä»¬æ”¹è¿›ç³»ç»Ÿ")
                st.session_state.show_feedback = False
            except Exception as e:
                st.error(f"åé¦ˆä¿å­˜å¤±è´¥: {str(e)}")

# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    check_launch()
    main_application()
