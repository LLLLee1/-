import streamlit as st
# å¿…é¡»ä½œä¸ºç¬¬ä¸€ä¸ªStreamlitå‘½ä»¤
st.set_page_config(
    page_title="AIæ–°é—»çœŸå®æ€§æ£€æµ‹ç³»ç»Ÿ",
    page_icon="ğŸ”",
    layout="wide",
    menu_items={
        'Get Help': 'https://example.com',
        'Report a bug': "https://example.com",
        'About': "# æ–°é—»çœŸå®æ€§æ£€æµ‹ç³»ç»Ÿ v6.0\nä½¿ç”¨å…ˆè¿›çš„NLPä¸è¯­ä¹‰åˆ†ææŠ€æœ¯è¯„ä¼°æ–°é—»å¯ä¿¡åº¦"
    }
)

# å…¶ä»–å¯¼å…¥
import joblib
import numpy as np
import os
import re
import jieba
import pandas as pd
import json
from datetime import datetime
import hashlib
import math

# é…ç½®jiebaè¯å…¸ï¼ˆç¡®ä¿åœ¨å‡½æ•°å¤–éƒ¨ï¼‰
if os.path.exists('dict.txt.big'):
    jieba.set_dictionary('dict.txt.big')

# å¯åŠ¨éªŒè¯
def check_launch():
    """å®‰å…¨æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯"""
    with st.container():
        st.title("ğŸ” AIæ–°é—»çœŸå®æ€§æ£€æµ‹ç³»ç»Ÿ")
        st.markdown("ä½¿ç”¨æ·±åº¦å­¦ä¹ ä¸è¯­ä¹‰åˆ†ææŠ€æœ¯ç²¾å‡†è¯„ä¼°æ–°é—»å¯ä¿¡åº¦")
        st.divider()
        st.caption(f"ç³»ç»Ÿç‰ˆæœ¬: 6.0 | æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d')}")

# æ¨¡å‹åŠ è½½å‡½æ•°
@st.cache_resource(show_spinner="åŠ è½½æ¨¡å‹ç»„ä»¶ä¸­...")
def load_models():
    """å®‰å…¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ç»„ä»¶"""
    models = {}
    model_dir = "models"
    
    try:
        # å®‰å…¨åŠ è½½TF-IDFæ¨¡å‹
        tfidf_path = os.path.join(model_dir, "tfidf_vectorizer_enhanced.pkl")
        if os.path.exists(tfidf_path):
            models["tfidf"] = joblib.load(tfidf_path)
        else:
            st.toast("âš ï¸ TF-IDFæ¨¡å‹æœªæ‰¾åˆ°ï¼Œä½¿ç”¨ç®€åŒ–ç‰¹å¾æå–", icon="âš ï¸")
        
        # åŠ è½½åˆ†ç±»å™¨
        classifier_path = os.path.join(model_dir, "enhanced_fake_news_model.pkl")
        if os.path.exists(classifier_path):
            models["classifier"] = joblib.load(classifier_path)
        
        # è®¾ç½®æœŸæœ›ç»´åº¦
        models["expected_dim"] = 204
        
        # åŠ è½½è¯å…¸
        custom_dict_path = os.path.join(model_dir, "custom_dict.txt")
        if os.path.exists(custom_dict_path):
            jieba.load_userdict(custom_dict_path)
        
        return models
    
    except Exception as e:
        st.exception(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return {"error": str(e)}

# æ–‡æœ¬é¢„å¤„ç†å‡½æ•° - å¢å¼ºç‰ˆï¼ˆå«å®‰å…¨è¾¹ç•Œï¼‰
def preprocess_text(text, max_length=5000):
    """å®‰å…¨é¢„å¤„ç†æ–‡æœ¬"""
    if not text or len(text) > max_length:
        return ""
    
    # å®‰å…¨ç§»é™¤ç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\.,ï¼Œã€‚!?ï¼›;:\-\'\"\n]', ' ', text)
    text = re.sub(r'\s+', ' ', text).lower()[:max_length]
    
    # å®‰å…¨åˆ†è¯å¤„ç†
    try:
        words = jieba.lcut(text, cut_all=False)
        return " ".join(words).strip()
    except:
        return text[:max_length]  # é™çº§å¤„ç†

# å¢å¼ºç‰¹å¾å·¥ç¨‹ï¼ˆå«æ•°å€¼è¾¹ç•Œï¼‰
def extract_text_features(text, max_length=10000):
    """å®‰å…¨æå–å·®å¼‚åŒ–ç‰¹å¾"""
    if not text:
        return {}
    
    # é™åˆ¶æ–‡æœ¬é•¿åº¦é˜²æ­¢æº¢å‡º
    text = text[:max_length]
    text_length = len(text)
    
    features = {
        'length': text_length,
        'sentences': min(100, text.count('ã€‚') + text.count('!') + text.count('?') + text.count(';') + 1),
        'commas': min(200, text.count(',') + text.count('ï¼Œ')),
        'exclamation': min(50, text.count('!') + text.count('ï¼')),
    }
    
    # 1. å¯ä¿¡åº¦ä¿¡å·ï¼ˆå¢å¼ºå·®å¼‚ï¼‰
    reliable_terms = ['æ–°åç¤¾', 'äººæ°‘æ—¥æŠ¥', 'å¤®è§†', 'çœæ”¿åºœ', 'æ•™è‚²éƒ¨', 'å«å¥å§”', 'å®˜æ–¹å£°æ˜']
    features['reliable_score'] = min(1.0, sum(1 for term in reliable_terms if term in text) * 0.2)
    
    # 2. æƒ…æ„Ÿåˆ†æç‰¹å¾
    positive_terms = ['æˆåŠŸ', 'è¿›å±•', 'æˆå°±', 'çªç ´', 'åº†ç¥', 'åˆä½œ']
    negative_terms = ['å¤±è´¥', 'äº‰è®®', 'ä¸‘é—»', 'å†²çª', 'è­¦å‘Š', 'è°´è´£']
    positive_count = sum(1 for term in positive_terms if term in text)
    negative_count = sum(1 for term in negative_terms if term in text)
    features['positive_sentiment'] = min(1.0, positive_count / max(1, text_length/50))
    features['negative_sentiment'] = min(1.0, negative_count / max(1, text_length/50))
    
    # 3. ç»“æ„ç‰¹å¾ï¼ˆå«è¾¹ç•Œæ£€æŸ¥ï¼‰
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ!?]', text)
    long_sentences = sum(1 for s in sentences if len(s) > 50)
    features['long_sentence_ratio'] = min(1.0, long_sentences / max(1, features['sentences']))
    
    # 4. æ•°å­—/æ•°æ®ç‰¹å¾
    data_terms = ['%', 'äº¿å…ƒ', 'ä¸‡äºº', 'å…¬æ–¤', 'å…¬é‡Œ', 'ç ”ç©¶å‘ç°', 'æ•°æ®è¡¨æ˜', 'æ®ç»Ÿè®¡']
    features['data_presence'] = 1 if any(term in text for term in data_terms) else 0
    features['number_count'] = min(1.0, len(re.findall(r'\d+', text)) / max(1, text_length/100))
    
    # 5. å¼‚å¸¸å†…å®¹æ£€æµ‹
    absurd_terms = ['æœˆçƒçˆ†ç‚¸', 'å¤ªé˜³æ¶ˆå¤±', 'åœ°çƒåœè½¬', 'é•¿ç”Ÿä¸è€', 'æ—¶å…‰å€’æµ', 'å¤–æ˜Ÿäºº']
    features['absurd_score'] = min(2.0, sum(0.5 for term in absurd_terms if term in text))
    
    # 6. å¯ç–‘è¯­è¨€ç‰¹å¾
    urgency_terms = ['å¿…çœ‹', 'ç´§æ€¥', 'é€Ÿçœ‹', 'èµ¶å¿«', 'ç«‹åˆ»']
    exaggeration_terms = ['éœ‡æƒŠ', 'æœ€ç‰›', '100%', 'æƒŠå¤©', 'ç§˜é—»', 'ç»å¯†']
    features['urgency_score'] = min(1.0, sum(0.3 for term in urgency_terms if term in text))
    features['exaggeration_score'] = min(1.0, sum(0.4 for term in exaggeration_terms if term in text))
    
    # 7. å†…å®¹ç‹¬ç‰¹æ€§ç‰¹å¾ï¼ˆå®‰å…¨å“ˆå¸Œï¼‰
    try:
        content_hash = int(hashlib.sha256(text.encode()).hexdigest(), 16) % 10000
        features['uniqueness'] = (content_hash % 100) / 100
    except:
        features['uniqueness'] = 0.5  # é»˜è®¤å€¼
    
    return features

# å¢å¼ºç‰¹å¾ç”Ÿæˆå‡½æ•°ï¼ˆå«ç»´åº¦ä¿æŠ¤ï¼‰
def generate_features(text, models):
    """ç”Ÿæˆå®‰å…¨ç‰¹å¾å‘é‡"""
    # ç©ºè¾“å…¥ä¿æŠ¤
    if not text.strip():
        return np.zeros((1, models.get("expected_dim", 204)), dtype=np.float32)
    
    processed_text = preprocess_text(text)
    
    try:
        # TF-IDFç‰¹å¾
        tfidf_vector = np.zeros((1, 1))
        if "tfidf" in models and models["tfidf"]:
            tfidf_vector = models["tfidf"].transform([processed_text]).toarray()
            if tfidf_vector.shape[1] > models["expected_dim"]:
                tfidf_vector = tfidf_vector[:, :models["expected_dim"]]
        
        # é«˜çº§ç‰¹å¾
        adv_features = extract_text_features(text)
        feature_keys = sorted(adv_features.keys())
        adv_vector = [float(adv_features[key]) for key in feature_keys]
        
        # åˆå¹¶ç‰¹å¾ï¼ˆä¿æŠ¤ç»´åº¦ï¼‰
        combined = np.concatenate([tfidf_vector[0], adv_vector])
        if len(combined) < models["expected_dim"]:
            padded = np.zeros(models["expected_dim"])
            padded[:len(combined)] = combined
            return padded.reshape(1, -1)
        elif len(combined) > models["expected_dim"]:
            return combined[:models["expected_dim"]].reshape(1, -1)
        else:
            return combined.reshape(1, -1)
    
    except Exception as e:
        st.toast(f"ç‰¹å¾ç”Ÿæˆé”™è¯¯: {str(e)}", icon="âš ï¸")
        return np.zeros((1, models.get("expected_dim", 204)), dtype=np.float32)

# ä¸“ä¸šå¼‚å¸¸å†…å®¹è¯†åˆ«ç³»ç»Ÿ - å®‰å…¨ç‰ˆ
def absurd_content_detector(text, max_length=10000):
    """å®‰å…¨è¯†åˆ«è’è°¬å†…å®¹"""
    if not text:
        return False, ""
    
    # é™åˆ¶æ–‡æœ¬é•¿åº¦
    text = text[:max_length]
    
    # 1. è¿åç§‘å­¦è§„å¾‹
    absurd_patterns = [
        ('æœˆçƒçˆ†ç‚¸', 'å¤©ä½“æ¯ç­'),
        ('å¤ªé˜³æ¶ˆå¤±', 'å¤©ä½“æ¶ˆå¤±'),
        ('åœ°çƒåœè½¬', 'è¿åç‰©ç†è§„å¾‹'),
        ('é•¿ç”Ÿä¸è€', 'è¿åç”Ÿç‰©è§„å¾‹'),
        ('æ—¶å…‰å€’æµ', 'è¿åç‰©ç†è§„å¾‹')
    ]
    
    for phrase, reason in absurd_patterns:
        if phrase in text:
            return True, f"æ£€æµ‹åˆ°è’è°¬å†…å®¹: {reason}"
    
    # 2. æç«¯ç»Ÿè®¡æ•°æ®
    if re.search(r'99\.9[%ï¼…]çš„äºº|99\.9[%ï¼…]éƒ½|æ‰€æœ‰äºº|å®Œå…¨æ¶ˆé™¤', text):
        return True, "æ£€æµ‹åˆ°æç«¯ç»Ÿè®¡æ•°æ®"
    
    # 3. ä¸å¯èƒ½äº‹ä»¶
    if re.search(r'ä¸€å¤œä¹‹é—´|ç¬é—´è§£å†³|å®Œå…¨æ”¹å˜', text):
        return True, "æ£€æµ‹åˆ°ä¸åˆå¸¸ç†çš„æ—¶é—´è¡¨è¿°"
    
    return False, ""

# ç½®ä¿¡åº¦è®¡ç®—ç³»ç»Ÿï¼ˆå«æ•°å€¼ä¿æŠ¤ï¼‰
def calculate_confidence(probabilities, text):
    """å®‰å…¨è®¡ç®—ç½®ä¿¡åº¦"""
    # ç©ºæ¦‚ç‡å¤„ç†
    if not probabilities.size or probabilities.shape[1] < 2:
        return 0.5, 0.5, []
    
    # åˆå§‹æ¦‚ç‡
    real_prob = max(0.01, min(0.99, probabilities[0][0]))
    fake_prob = max(0.01, min(0.99, probabilities[0][1]))
    
    # ç©ºæ–‡æœ¬ä¿æŠ¤
    if not text:
        return real_prob, fake_prob, []
    
    # æå–ç‰¹å¾
    features = extract_text_features(text)
    
    # å¯ä¿¡åº¦è°ƒæ•´å› å­
    credibility_factors = []
    
    # 1. æƒå¨æ¥æºæå‡çœŸå®æ¦‚ç‡
    if features.get('reliable_score', 0) > 0:
        boost = min(0.2, features['reliable_score'] * 0.5)
        real_prob = min(0.99, real_prob + boost)
        credibility_factors.append(f"æƒå¨æ¥æº +{int(boost*100)}%")
    
    # 2. æ•°æ®æ”¯æ’‘æå‡çœŸå®æ¦‚ç‡
    if features.get('data_presence', 0):
        real_prob = min(0.95, real_prob + 0.1)
        credibility_factors.append("æ•°æ®æ”¯æŒ +10%")
    
    # 3. å¼‚å¸¸å†…å®¹æå‡è™šå‡æ¦‚ç‡
    if features.get('absurd_score', 0) > 0:
        fake_boost = min(0.35, features['absurd_score'] * 0.7)
        fake_prob = min(0.99, fake_prob + fake_boost)
        credibility_factors.append(f"å¼‚å¸¸å†…å®¹ +{int(fake_boost*100)}%")
    
    # 4. æƒ…æ„Ÿå› ç´ è°ƒæ•´
    sentiment_diff = features.get('positive_sentiment', 0) - features.get('negative_sentiment', 0)
    if sentiment_diff < -0.1:  # è´Ÿé¢æƒ…æ„Ÿ
        adjustment = min(0.15, abs(sentiment_diff) * 0.5)
        fake_prob = min(0.99, fake_prob + adjustment)
        credibility_factors.append(f"è´Ÿé¢æƒ…ç»ª +{int(adjustment*100)}%")
    
    # 5. é•¿åº¦å› ç´ è°ƒæ•´
    text_length = features.get('length', 0)
    if 200 <= text_length <= 500:  # æœ€ä½³é•¿åº¦
        adjustment = min(0.15, 0.05 + text_length * 0.0002)
        real_prob = min(0.99, real_prob + adjustment)
        credibility_factors.append(f"ç†æƒ³é•¿åº¦ +{int(adjustment*100)}%")
    elif text_length < 50 or text_length > 1000:  # è¿‡é•¿æˆ–è¿‡çŸ­
        fake_prob = min(0.99, fake_prob + 0.1)
        real_prob = max(0.01, real_prob - 0.1)
        credibility_factors.append("æ–‡æœ¬é•¿åº¦å¼‚å¸¸ Â±10%")
    
    # 6. é•¿å¥æ¯”ä¾‹å½±å“
    if features.get('long_sentence_ratio', 0) > 0.3:
        adjustment = min(0.15, features['long_sentence_ratio'] * 0.4)
        fake_prob = min(0.99, fake_prob + adjustment)
        credibility_factors.append(f"å¤æ‚å¥å¼ +{int(adjustment*100)}%")
    
    # 7. å†…å®¹ç‹¬ç‰¹æ€§
    uniqueness = features.get('uniqueness', 0.5)
    if uniqueness > 0.7:
        adjustment = min(0.1, uniqueness * 0.2)
        real_prob = min(0.99, real_prob + adjustment)
        credibility_factors.append(f"ç‹¬ç‰¹å†…å®¹ +{int(adjustment*100)}%")
    
    # 8. å¯ç–‘è¯­è¨€ç‰¹å¾
    urgency_score = features.get('urgency_score', 0)
    exaggeration_score = features.get('exaggeration_score', 0)
    if urgency_score > 0 or exaggeration_score > 0:
        adjustment = min(0.25, (urgency_score + exaggeration_score) * 0.3)
        fake_prob = min(0.99, fake_prob + adjustment)
        credibility_factors.append(f"å¯ç–‘è¯­è¨€ +{int(adjustment*100)}%")
    
    # å½’ä¸€åŒ–å¤„ç†
    total = real_prob + fake_prob
    return real_prob/total, fake_prob/total, credibility_factors

# ç”Ÿæˆå®‰å…¨æŠ¥å‘Š
def generate_report(text, real_prob, fake_prob, is_absurd, credibility_factors):
    """ç”Ÿæˆå®‰å…¨åˆ†ææŠ¥å‘Š"""
    if not text:
        return {
            "summary": "è¾“å…¥å†…å®¹ä¸ºç©º",
            "key_insights": ["æœªæä¾›æœ‰æ•ˆå†…å®¹"],
            "recommendations": ["è¯·è¾“å…¥æ–°é—»å†…å®¹åé‡è¯•"]
        }
    
    try:
        features = extract_text_features(text)
        credibility_score = int(real_prob * 100)
        
        risk_level = "é«˜é£é™©"
        color = "red"
        if credibility_score >= 85:
            risk_level = "ä½é£é™©"
            color = "green"
        elif credibility_score >= 70:
            risk_level = "ä¸­ç­‰é£é™©"
            color = "orange"
        
        report = {
            "summary": f"å¯ä¿¡åº¦è¯„åˆ†: {credibility_score}/100",
            "risk_level": f"<span style='color:{color};font-weight:bold'>{risk_level}</span>",
            "key_insights": [],
            "recommendations": [
                "å»ºè®®æ ¸å®ä¿¡æ¯çš„åŸå§‹æ¥æº",
                "æ£€æŸ¥å¤šä¸ªç‹¬ç«‹ä¿¡æ¯æºè¿›è¡Œäº¤å‰éªŒè¯"
            ]
        }
        
        # å…³é”®æ´å¯Ÿ
        if is_absurd:
            report["key_insights"].append("â›” æ£€æµ‹åˆ°é«˜åº¦è’è°¬å†…å®¹")
        if features['reliable_score'] > 0:
            report["key_insights"].append(f"âœ… æ£€æµ‹åˆ°{features['reliable_score']:.1f}ä¸ªæƒå¨æ¥æº")
        else:
            report["key_insights"].append("âš ï¸ æœªæ£€æµ‹åˆ°æƒå¨æ¥æº")
        if features['data_presence']:
            report["key_insights"].append("ğŸ“Š åŒ…å«æ•°æ®æ”¯æ’‘")
        else:
            report["key_insights"].append("â„¹ï¸ ç¼ºä¹å…·ä½“æ•°æ®")
        if features['exaggeration_score'] > 0.5:
            report["key_insights"].append("â— æ£€æµ‹åˆ°å¤¸å¼ è¯­è¨€")
        
        return report
    except:
        return {
            "summary": "æŠ¥å‘Šç”Ÿæˆå‡ºé”™",
            "key_insights": ["ç³»ç»Ÿå†…éƒ¨é”™è¯¯"],
            "recommendations": ["è¯·å°è¯•é‡æ–°è¾“å…¥å†…å®¹"]
        }

# ç”¨æˆ·ç•Œé¢ä¸»å‡½æ•° - å®‰å…¨å¢å¼ºç‰ˆ
def main_application():
    # å®‰å…¨åˆå§‹åŒ–
    st.session_state.setdefault("show_feedback", False)
    st.session_state.setdefault("history", [])
    st.session_state.setdefault("last_result", {})
    
    # å®‰å…¨ä¾§è¾¹æ 
    with st.sidebar:
        try:
            st.markdown("### ğŸ› ï¸ ç³»ç»ŸçŠ¶æ€")
            models = load_models()
            if "error" in models:
                st.warning("æ¨¡å‹åŠ è½½å¼‚å¸¸ - ä½¿ç”¨ç®€åŒ–æ¨¡å¼")
            else:
                st.success("âœ“ æ¨¡å‹å°±ç»ª")
                st.info(f"ç‰¹å¾ç»´åº¦: {models.get('expected_dim', 'N/A')}")
            
            st.divider()
            st.markdown("### ğŸ“ ä½¿ç”¨æŒ‡å—")
            st.markdown("""
            1. ç²˜è´´æ–°é—»å†…å®¹ï¼ˆ200å­—+æ›´å‡†ç¡®ï¼‰
            2. é¿å…å•å¥æ£€æµ‹
            3. æƒå¨æ¥æºæé«˜å¯ä¿¡åº¦
            4. æ¯æ¬¡æ£€æµ‹ç»“æœéƒ½ä¼šå˜åŒ–
            """)
            
            # å†å²è®°å½•ï¼ˆå®‰å…¨æ˜¾ç¤ºï¼‰
            st.divider()
            st.markdown("### ğŸ•’ å†å²æ£€æµ‹")
            if st.session_state.history:
                for i, item in list(enumerate(st.session_state.history[-3:])):
                    icon = "âœ…" if item.get("credibility", 0) > 70 else "âš ï¸" if item.get("credibility", 0) > 40 else "âŒ"
                    st.caption(f"{icon} {item.get('time', '')} - {item.get('result', '')}")
            else:
                st.caption("æ— å†å²è®°å½•")
            
            st.divider()
            if st.button("ğŸ“¨ æŠ¥å‘Šé—®é¢˜", use_container_width=True):
                st.session_state.show_feedback = True
        except:
            st.warning("ä¾§è¾¹æ åˆå§‹åŒ–å‡ºé”™")

    # ä¸»ç•Œé¢ï¼ˆå«å¼‚å¸¸æ•è·ï¼‰
    try:
        st.header("ğŸ“° æ–°é—»çœŸå®æ€§åˆ†æ")
        st.caption("ç²˜è´´æ–°é—»å†…å®¹è·å–ç²¾å‡†å¯ä¿¡åº¦è¯„ä¼°")
        
        news_text = st.text_area("æ–°é—»å†…å®¹:", 
                                height=250, 
                                placeholder="åœ¨æ­¤å¤„ç²˜è´´æ–°é—»å†…å®¹...",
                                help="æ”¯æŒä¸­è‹±æ–‡å†…å®¹ï¼Œæœ€ä½³é•¿åº¦200-1000å­—",
                                key="news_input")
        
        if st.button("âœ… æ£€æµ‹çœŸå®æ€§", type="primary", use_container_width=True):
            if not news_text.strip():
                st.warning("è¯·è¾“å…¥æ–°é—»å†…å®¹")
                return
                
            progress_bar = st.progress(0, text="åˆ†æå‡†å¤‡ä¸­...")
            
            try:
                # 1. è’è°¬å†…å®¹æ£€æµ‹
                progress_bar.progress(15, "æ£€æµ‹å¼‚å¸¸å†…å®¹...")
                is_absurd, reason = absurd_content_detector(news_text)
                if is_absurd:
                    st.error(f"â›” **é«˜é£é™©è™šå‡æ–°é—»** - {reason}")
                    progress_bar.progress(100)
                    
                    # ä¿å­˜å†å²
                    st.session_state.history.append({
                        "time": datetime.now().strftime("%H:%M"),
                        "text": news_text[:100] + "..." if len(news_text) > 100 else news_text,
                        "result": "è’è°¬å†…å®¹ - è™šå‡æ–°é—»",
                        "credibility": 10
                    })
                    
                    # ç”ŸæˆæŠ¥å‘Š
                    with st.expander("ğŸ“Š è¯¦ç»†åˆ†ææŠ¥å‘Š", expanded=True):
                        report = generate_report(news_text, 0.1, 0.9, True, [])
                        st.subheader("å¯ä¿¡åº¦è¯„ä¼°")
                        st.markdown(f"### {report['summary']}")
                        st.markdown(f"**é£é™©ç­‰çº§**: {report['risk_level']}", unsafe_allow_html=True)
                        st.subheader("å…³é”®æ´å¯Ÿ")
                        for insight in report["key_insights"]:
                            st.write(f"- {insight}")
                        st.subheader("å»ºè®®")
                        for rec in report["recommendations"]:
                            st.info(f"- {rec}")
                    
                    return
                
                # 2. ç‰¹å¾æå–
                progress_bar.progress(35, "æå–å†…å®¹ç‰¹å¾...")
                features = generate_features(news_text, models)
                
                # 3. æ¨¡å‹é¢„æµ‹ï¼ˆå®‰å…¨é™çº§ï¼‰
                progress_bar.progress(65, "åˆ†æå†…å®¹çœŸå®æ€§...")
                if "classifier" in models and models["classifier"]:
                    probabilities = models["classifier"].predict_proba(features)
                else:
                    # è§„åˆ™é™çº§
                    fake_indicators = features[0][-8:].sum() - 2
                    probabilities = np.array([[0.5, 0.5] if fake_indicators < 0 else [0.3, 0.7]])
                
                # 4. ç½®ä¿¡åº¦è®¡ç®—
                progress_bar.progress(85, "è®¡ç®—å¯ä¿¡åº¦...")
                real_prob, fake_prob, factors = calculate_confidence(probabilities, news_text)
                
                # 5. æ˜¾ç¤ºç»“æœ
                result_type = "é«˜é£é™© - å¯ç–‘å†…å®¹" if fake_prob > 0.6 else "å¯é  - çœŸå®å†…å®¹"
                credibility_score = fake_prob * 100 if fake_prob > 0.6 else real_prob * 100
                
                if fake_prob > 0.6:
                    st.error(f"âš ï¸ **å¯ç–‘æ–°é—»** (é£é™©: {fake_prob*100:.1f}%)")
                else:
                    st.success(f"âœ… **å¯é æ–°é—»** (å¯ä¿¡åº¦: {real_prob*100:.1f}%)")
                
                progress_bar.progress(100)
                
                # å¯è§†åŒ–
                col1, col2 = st.columns([1, 2])
                with col1:
                    st.metric("å¯ä¿¡åº¦", f"{real_prob*100:.1f}%", delta_color="inverse")
                    st.metric("è™šå‡é£é™©", f"{fake_prob*100:.1f}%", delta_color="inverse")
                
                # è¯¦ç»†æŠ¥å‘Š
                with st.expander("ğŸ“Š è¯¦ç»†åˆ†ææŠ¥å‘Š", expanded=True):
                    report = generate_report(news_text, real_prob, fake_prob, False, factors)
                    st.subheader("å¯ä¿¡åº¦è¯„ä¼°")
                    st.markdown(f"### {report['summary']}")
                    st.markdown(f"**é£é™©ç­‰çº§**: {report['risk_level']}", unsafe_allow_html=True)
                    
                    if factors:
                        st.subheader("å…³é”®å½±å“å› ç´ ")
                        for factor in factors:
                            st.caption(f"- {factor}")
                    
                    st.subheader("å†…å®¹åˆ†æ")
                    for insight in report["key_insights"]:
                        st.write(f"- {insight}")
                    
                    st.subheader("ä¸“å®¶å»ºè®®")
                    for rec in report["recommendations"]:
                        st.info(f"- {rec}")
                
                # ä¿å­˜ç»“æœ
                st.session_state.last_result = {
                    "text": news_text, 
                    "result": result_type,
                    "confidence": credibility_score
                }
                st.session_state.history.append({
                    "time": datetime.now().strftime("%H:%M"),
                    "text": news_text[:100] + "..." if len(news_text) > 100 else news_text,
                    "result": result_type,
                    "credibility": round(credibility_score, 1)
                })
                
            except Exception as e:
                progress_bar.progress(100)
                st.error(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
                st.exception(e)
        
        # åé¦ˆç³»ç»Ÿ
        if st.session_state.get("show_feedback", False):
            st.divider()
            st.subheader("âœ‰ï¸ æŠ¥å‘Šé—®é¢˜")
            
            with st.form(key="feedback_form"):
                st.radio("ç»“æœå‡†ç¡®æ€§:", 
                        ("éå¸¸å‡†ç¡®", "éƒ¨åˆ†å‡†ç¡®", "ä¸å‡†ç¡®"), 
                        index=1,
                        key="feedback_accuracy")
                
                st.radio("å®é™…æ–°é—»ç±»å‹:", 
                       ("çœŸå®æ–°é—»", "ä¸ç¡®å®š", "è™šå‡æ–°é—»"), 
                       index=1,
                       key="feedback_label")
                
                st.text_area("é—®é¢˜æè¿°:", 
                           height=100,
                           placeholder="è¯·æè¿°æ‚¨å‘ç°çš„é—®é¢˜...",
                           key="feedback_comment")
                
                if st.form_submit_button("æäº¤åé¦ˆ"):
                    try:
                        feedback_dir = "user_feedback"
                        os.makedirs(feedback_dir, exist_ok=True)
                        
                        feedback_data = {
                            "time": datetime.now().isoformat(),
                            "text": st.session_state.get("last_result", {}).get("text", "")[:500],
                            "system_result": st.session_state.get("last_result", {}).get("result", ""),
                            "system_confidence": st.session_state.get("last_result", {}).get("confidence", 0),
                            "user_accuracy": st.session_state.feedback_accuracy,
                            "user_label": st.session_state.feedback_label,
                            "comment": st.session_state.feedback_comment
                        }
                        
                        with open(os.path.join(feedback_dir, "feedback.jsonl"), "a", encoding="utf-8") as f:
                            f.write(json.dumps(feedback_data, ensure_ascii=False) + "\n")
                        
                        st.success("æ„Ÿè°¢åé¦ˆï¼æ‚¨çš„æ„è§éå¸¸é‡è¦")
                        st.session_state.show_feedback = False
                    except:
                        st.error("åé¦ˆä¿å­˜å¤±è´¥")
    
    except Exception as e:
        st.error(f"ä¸»ç•Œé¢åˆå§‹åŒ–å‡ºé”™: {str(e)}")

# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    try:
        check_launch()
        main_application()
    except Exception as e:
        st.error(f"ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {str(e)}")
        st.error("è¯·è”ç³»æŠ€æœ¯äººå‘˜å¹¶æä¾›æ­¤é”™è¯¯ä¿¡æ¯")
